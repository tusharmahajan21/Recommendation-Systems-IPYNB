{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE(20-10-10-20).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k_o2GGXbsHg",
        "colab_type": "text"
      },
      "source": [
        "Basic AE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1_S6g9VHcdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfxKXF3RHeK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "movies = pd.read_csv('movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csbpbvQwJBPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the training set and the test set\n",
        "training_set = pd.read_csv('u1.base', delimiter = '\\t')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv('u1.test', delimiter = '\\t')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRGn0tM8JEvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhtPeh9LJINI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the data into an array with users in lines and movies in columns\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxL66kDMJLSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the data into Torch tensors\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQtU2P5jJNiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZkyLBICJR9J",
        "colab_type": "code",
        "outputId": "bcea1b52-e63a-4f11-8f71-4722f542a840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nb_epoch = 256\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 1.7654765648303454\n",
            "epoch: 2 loss: 1.0965477681691367\n",
            "epoch: 3 loss: 1.053330380601304\n",
            "epoch: 4 loss: 1.038332857428127\n",
            "epoch: 5 loss: 1.030750021635602\n",
            "epoch: 6 loss: 1.0265925306777666\n",
            "epoch: 7 loss: 1.023716315002472\n",
            "epoch: 8 loss: 1.0220328615398064\n",
            "epoch: 9 loss: 1.020795714252618\n",
            "epoch: 10 loss: 1.0193805351847074\n",
            "epoch: 11 loss: 1.0188264644225673\n",
            "epoch: 12 loss: 1.018606546893516\n",
            "epoch: 13 loss: 1.0177031450680463\n",
            "epoch: 14 loss: 1.0176577353721896\n",
            "epoch: 15 loss: 1.0171402143978208\n",
            "epoch: 16 loss: 1.0166849397574707\n",
            "epoch: 17 loss: 1.0162828272316218\n",
            "epoch: 18 loss: 1.0165633464566888\n",
            "epoch: 19 loss: 1.0162155437992317\n",
            "epoch: 20 loss: 1.0164159705128448\n",
            "epoch: 21 loss: 1.0159995961059702\n",
            "epoch: 22 loss: 1.0159736021635695\n",
            "epoch: 23 loss: 1.0157907324580249\n",
            "epoch: 24 loss: 1.0158396838416663\n",
            "epoch: 25 loss: 1.0156797073243824\n",
            "epoch: 26 loss: 1.015732387128642\n",
            "epoch: 27 loss: 1.015497324158516\n",
            "epoch: 28 loss: 1.0149795444640695\n",
            "epoch: 29 loss: 1.0126313791021395\n",
            "epoch: 30 loss: 1.0112833362381342\n",
            "epoch: 31 loss: 1.010347781751798\n",
            "epoch: 32 loss: 1.0081127577694187\n",
            "epoch: 33 loss: 1.008235412382365\n",
            "epoch: 34 loss: 1.0038840610927937\n",
            "epoch: 35 loss: 1.0025014093995224\n",
            "epoch: 36 loss: 1.0010494470781803\n",
            "epoch: 37 loss: 0.9984738925796794\n",
            "epoch: 38 loss: 0.9949676274602421\n",
            "epoch: 39 loss: 0.9944451918051657\n",
            "epoch: 40 loss: 0.990714455176645\n",
            "epoch: 41 loss: 0.990997796187433\n",
            "epoch: 42 loss: 0.9892964063244756\n",
            "epoch: 43 loss: 0.9868238562205828\n",
            "epoch: 44 loss: 0.9822078779842329\n",
            "epoch: 45 loss: 0.9862803790877658\n",
            "epoch: 46 loss: 0.9838081337763839\n",
            "epoch: 47 loss: 0.9804572269792721\n",
            "epoch: 48 loss: 0.9776006026707386\n",
            "epoch: 49 loss: 0.9790582441364063\n",
            "epoch: 50 loss: 0.9739307628660527\n",
            "epoch: 51 loss: 0.9720674709341164\n",
            "epoch: 52 loss: 0.9702710865445963\n",
            "epoch: 53 loss: 0.9742473001589762\n",
            "epoch: 54 loss: 0.9727441946434388\n",
            "epoch: 55 loss: 0.9699046075499045\n",
            "epoch: 56 loss: 0.9667753523127123\n",
            "epoch: 57 loss: 0.97048100563192\n",
            "epoch: 58 loss: 0.9625994078246386\n",
            "epoch: 59 loss: 0.9635169544729033\n",
            "epoch: 60 loss: 0.9608267138684623\n",
            "epoch: 61 loss: 0.9618749459115746\n",
            "epoch: 62 loss: 0.956300264475586\n",
            "epoch: 63 loss: 0.9593445142556897\n",
            "epoch: 64 loss: 0.9566989149683701\n",
            "epoch: 65 loss: 0.9579726468217681\n",
            "epoch: 66 loss: 0.9524575825114451\n",
            "epoch: 67 loss: 0.9532909622033456\n",
            "epoch: 68 loss: 0.9500614715664754\n",
            "epoch: 69 loss: 0.9522710153407026\n",
            "epoch: 70 loss: 0.9495663301583568\n",
            "epoch: 71 loss: 0.9500334062372217\n",
            "epoch: 72 loss: 0.9461395055613508\n",
            "epoch: 73 loss: 0.9479941302609403\n",
            "epoch: 74 loss: 0.9466064467379087\n",
            "epoch: 75 loss: 0.947025956966133\n",
            "epoch: 76 loss: 0.9432695873820661\n",
            "epoch: 77 loss: 0.9445150363659327\n",
            "epoch: 78 loss: 0.9420248014897553\n",
            "epoch: 79 loss: 0.9440468630500407\n",
            "epoch: 80 loss: 0.9422915069505191\n",
            "epoch: 81 loss: 0.9428549495686892\n",
            "epoch: 82 loss: 0.9404973228312127\n",
            "epoch: 83 loss: 0.9404099978042336\n",
            "epoch: 84 loss: 0.9392602390954402\n",
            "epoch: 85 loss: 0.9395722333099034\n",
            "epoch: 86 loss: 0.9388269745114879\n",
            "epoch: 87 loss: 0.9386487847019416\n",
            "epoch: 88 loss: 0.9375656559038343\n",
            "epoch: 89 loss: 0.9376153578975539\n",
            "epoch: 90 loss: 0.9372987390377138\n",
            "epoch: 91 loss: 0.9366567356483426\n",
            "epoch: 92 loss: 0.9357141181319909\n",
            "epoch: 93 loss: 0.936115357889545\n",
            "epoch: 94 loss: 0.9352387889963553\n",
            "epoch: 95 loss: 0.9354041129068775\n",
            "epoch: 96 loss: 0.9349971277226286\n",
            "epoch: 97 loss: 0.9348561706658178\n",
            "epoch: 98 loss: 0.933606215803837\n",
            "epoch: 99 loss: 0.9337072518156337\n",
            "epoch: 100 loss: 0.9326120071494561\n",
            "epoch: 101 loss: 0.9333276529381345\n",
            "epoch: 102 loss: 0.9320024625394653\n",
            "epoch: 103 loss: 0.93262921899365\n",
            "epoch: 104 loss: 0.9308859525195564\n",
            "epoch: 105 loss: 0.9319758148349202\n",
            "epoch: 106 loss: 0.9312630698273278\n",
            "epoch: 107 loss: 0.9314004969849324\n",
            "epoch: 108 loss: 0.9305237899477239\n",
            "epoch: 109 loss: 0.9307911537739499\n",
            "epoch: 110 loss: 0.9295377655999647\n",
            "epoch: 111 loss: 0.9296855813798331\n",
            "epoch: 112 loss: 0.9281972283269482\n",
            "epoch: 113 loss: 0.9300198433539539\n",
            "epoch: 114 loss: 0.9288716570598189\n",
            "epoch: 115 loss: 0.9293895860256605\n",
            "epoch: 116 loss: 0.9283948414880031\n",
            "epoch: 117 loss: 0.9285785028904836\n",
            "epoch: 118 loss: 0.9274356744887854\n",
            "epoch: 119 loss: 0.9276736265974044\n",
            "epoch: 120 loss: 0.9266152507424013\n",
            "epoch: 121 loss: 0.9270628318618499\n",
            "epoch: 122 loss: 0.925955098490211\n",
            "epoch: 123 loss: 0.9268926072754143\n",
            "epoch: 124 loss: 0.9256358617144083\n",
            "epoch: 125 loss: 0.9258640285504143\n",
            "epoch: 126 loss: 0.9249407268731803\n",
            "epoch: 127 loss: 0.9256932138726574\n",
            "epoch: 128 loss: 0.9244685810716494\n",
            "epoch: 129 loss: 0.924771244736479\n",
            "epoch: 130 loss: 0.9237582367440469\n",
            "epoch: 131 loss: 0.9240703562251157\n",
            "epoch: 132 loss: 0.9229088784504087\n",
            "epoch: 133 loss: 0.9238247904915249\n",
            "epoch: 134 loss: 0.9225936209895628\n",
            "epoch: 135 loss: 0.9235102454662348\n",
            "epoch: 136 loss: 0.9223416678279397\n",
            "epoch: 137 loss: 0.9226155398456051\n",
            "epoch: 138 loss: 0.9216058316111848\n",
            "epoch: 139 loss: 0.9222137692295613\n",
            "epoch: 140 loss: 0.9211778705728966\n",
            "epoch: 141 loss: 0.9218223425047576\n",
            "epoch: 142 loss: 0.9205949620051005\n",
            "epoch: 143 loss: 0.9212901077933557\n",
            "epoch: 144 loss: 0.9201964519203304\n",
            "epoch: 145 loss: 0.9209732113134088\n",
            "epoch: 146 loss: 0.9198057612232201\n",
            "epoch: 147 loss: 0.9203413447762847\n",
            "epoch: 148 loss: 0.919226527957234\n",
            "epoch: 149 loss: 0.9199191830118626\n",
            "epoch: 150 loss: 0.9188301456895307\n",
            "epoch: 151 loss: 0.9196063581128191\n",
            "epoch: 152 loss: 0.9182391765831143\n",
            "epoch: 153 loss: 0.9187636645296055\n",
            "epoch: 154 loss: 0.9180162682666485\n",
            "epoch: 155 loss: 0.9188030699402897\n",
            "epoch: 156 loss: 0.9174495805785703\n",
            "epoch: 157 loss: 0.918120501172786\n",
            "epoch: 158 loss: 0.9172462085949671\n",
            "epoch: 159 loss: 0.9182167669058592\n",
            "epoch: 160 loss: 0.9173626126321959\n",
            "epoch: 161 loss: 0.9176268372697327\n",
            "epoch: 162 loss: 0.9166651364175352\n",
            "epoch: 163 loss: 0.9169072034243443\n",
            "epoch: 164 loss: 0.9165654536163809\n",
            "epoch: 165 loss: 0.9172088932164875\n",
            "epoch: 166 loss: 0.9162395540926179\n",
            "epoch: 167 loss: 0.9163833092493493\n",
            "epoch: 168 loss: 0.9159179390958079\n",
            "epoch: 169 loss: 0.9163314789010693\n",
            "epoch: 170 loss: 0.9159343667869287\n",
            "epoch: 171 loss: 0.9159374902465793\n",
            "epoch: 172 loss: 0.9153189730192026\n",
            "epoch: 173 loss: 0.9152830606883136\n",
            "epoch: 174 loss: 0.9149655691338682\n",
            "epoch: 175 loss: 0.9150936112290721\n",
            "epoch: 176 loss: 0.9148780638013997\n",
            "epoch: 177 loss: 0.9148155222596361\n",
            "epoch: 178 loss: 0.9144527689003294\n",
            "epoch: 179 loss: 0.9146337340754768\n",
            "epoch: 180 loss: 0.9142026584334458\n",
            "epoch: 181 loss: 0.913986783106679\n",
            "epoch: 182 loss: 0.913870384403121\n",
            "epoch: 183 loss: 0.913881995368834\n",
            "epoch: 184 loss: 0.9136967280834115\n",
            "epoch: 185 loss: 0.91375482109138\n",
            "epoch: 186 loss: 0.9134462977554765\n",
            "epoch: 187 loss: 0.9132207434589126\n",
            "epoch: 188 loss: 0.9129392515832093\n",
            "epoch: 189 loss: 0.9133959332630975\n",
            "epoch: 190 loss: 0.9127538799267159\n",
            "epoch: 191 loss: 0.9127819502808604\n",
            "epoch: 192 loss: 0.9126243654626763\n",
            "epoch: 193 loss: 0.912512240407425\n",
            "epoch: 194 loss: 0.9121074045369068\n",
            "epoch: 195 loss: 0.9126769514926629\n",
            "epoch: 196 loss: 0.9119610016063677\n",
            "epoch: 197 loss: 0.9121631874823782\n",
            "epoch: 198 loss: 0.9121506998661286\n",
            "epoch: 199 loss: 0.9118363281098536\n",
            "epoch: 200 loss: 0.9114430129453245\n",
            "epoch: 201 loss: 0.9115218156063833\n",
            "epoch: 202 loss: 0.9110400538890109\n",
            "epoch: 203 loss: 0.9112656261406485\n",
            "epoch: 204 loss: 0.9111776424789002\n",
            "epoch: 205 loss: 0.9111259368186132\n",
            "epoch: 206 loss: 0.9106116521447308\n",
            "epoch: 207 loss: 0.9106397151061066\n",
            "epoch: 208 loss: 0.9104216305960299\n",
            "epoch: 209 loss: 0.9100071732028654\n",
            "epoch: 210 loss: 0.9095766079362472\n",
            "epoch: 211 loss: 0.9097171743107952\n",
            "epoch: 212 loss: 0.9094382433537662\n",
            "epoch: 213 loss: 0.9095015150445932\n",
            "epoch: 214 loss: 0.9092937045016519\n",
            "epoch: 215 loss: 0.9094762815049618\n",
            "epoch: 216 loss: 0.9088736193596844\n",
            "epoch: 217 loss: 0.9087811657216814\n",
            "epoch: 218 loss: 0.9081433140074932\n",
            "epoch: 219 loss: 0.9082899262566243\n",
            "epoch: 220 loss: 0.9074093050643474\n",
            "epoch: 221 loss: 0.9078988583736534\n",
            "epoch: 222 loss: 0.9073444011741425\n",
            "epoch: 223 loss: 0.907279447584082\n",
            "epoch: 224 loss: 0.9064387142351534\n",
            "epoch: 225 loss: 0.9063059630727562\n",
            "epoch: 226 loss: 0.9058990304664787\n",
            "epoch: 227 loss: 0.9057011507721128\n",
            "epoch: 228 loss: 0.9053509100972024\n",
            "epoch: 229 loss: 0.9051580408962134\n",
            "epoch: 230 loss: 0.9045771021774078\n",
            "epoch: 231 loss: 0.9046556959242633\n",
            "epoch: 232 loss: 0.9040398587992794\n",
            "epoch: 233 loss: 0.9038190334041487\n",
            "epoch: 234 loss: 0.9033117017562238\n",
            "epoch: 235 loss: 0.9030520806966867\n",
            "epoch: 236 loss: 0.9025919364947431\n",
            "epoch: 237 loss: 0.90250558377871\n",
            "epoch: 238 loss: 0.902156636379235\n",
            "epoch: 239 loss: 0.9017475758378635\n",
            "epoch: 240 loss: 0.901200455542462\n",
            "epoch: 241 loss: 0.900887420620491\n",
            "epoch: 242 loss: 0.9005591009634059\n",
            "epoch: 243 loss: 0.9004745296099438\n",
            "epoch: 244 loss: 0.8998710933409682\n",
            "epoch: 245 loss: 0.8996806075417324\n",
            "epoch: 246 loss: 0.8989234243040342\n",
            "epoch: 247 loss: 0.8987490722705032\n",
            "epoch: 248 loss: 0.8984250474668761\n",
            "epoch: 249 loss: 0.898085342778121\n",
            "epoch: 250 loss: 0.8976235853631804\n",
            "epoch: 251 loss: 0.8969380311481145\n",
            "epoch: 252 loss: 0.8969485691568105\n",
            "epoch: 253 loss: 0.8967166154605528\n",
            "epoch: 254 loss: 0.8963205688729519\n",
            "epoch: 255 loss: 0.8956190831590484\n",
            "epoch: 256 loss: 0.8955293809382718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGjtxKpzJUq-",
        "colab_type": "code",
        "outputId": "38ad3506-5dbd-4a59-d4f7-299d8d462a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: 0.9487050695650937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TytBtasBJZzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u__HTDlw4TlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}