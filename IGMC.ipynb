{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MW8jRmeDz3Fq"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from data_utils import load_data, map_data, download_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIxC0eiv9Uo8"
   },
   "source": [
    "data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKkjjfHL0f8s"
   },
   "outputs": [],
   "source": [
    "def data_iterator(data, batch_size):\n",
    "    \"\"\"\n",
    "    A simple data iterator from https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "    :param data: list of numpy tensors that need to be randomly batched across their first dimension.\n",
    "    :param batch_size: int, batch_size of data_iterator.\n",
    "    Assumes same first dimension size of all numpy tensors.\n",
    "    :return: iterator over batches of numpy tensors\n",
    "    \"\"\"\n",
    "    # shuffle labels and features\n",
    "    max_idx = len(data[0])\n",
    "    idxs = np.arange(0, max_idx)\n",
    "    np.random.shuffle(idxs)\n",
    "    shuf_data = [dat[idxs] for dat in data]\n",
    "\n",
    "    # Does not yield last remainder of size less than batch_size\n",
    "    for i in range(max_idx//batch_size):\n",
    "        data_batch = [dat[i*batch_size:(i+1)*batch_size] for dat in shuf_data]\n",
    "        yield data_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gi3WlL8n0jB-"
   },
   "outputs": [],
   "source": [
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    data = np.array([id_dict[x] for x in data])\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFqgCjIG0mGP"
   },
   "outputs": [],
   "source": [
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "    num_items : int\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "    \n",
    "    if fname == 'ml_1m':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['ratings.dat', 'movies.dat', 'users.dat']\n",
    "        \n",
    "        sep = r'\\:\\:'\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "        data = pd.read_csv(filename, sep=sep, header=None,\n",
    "                           names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.values.tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "        ratings = ratings.astype(np.float32)\n",
    "\n",
    "        # Load movie features\n",
    "        movies_file = 'movies.dat'\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python')\n",
    "\n",
    "        # Extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # Creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # Load user features\n",
    "        users_file = 'users.dat'\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # Extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    # elif fname == 'ml_10m':\n",
    "\n",
    "    #     # Check if files exist and download otherwise\n",
    "    #     files = ['/ratings.dat']\n",
    "    #     download_dataset(fname, files, data_dir)\n",
    "\n",
    "    #     sep = r'\\:\\:'\n",
    "\n",
    "    #     filename = data_dir + files[0]\n",
    "\n",
    "    #     dtypes = {\n",
    "    #         'u_nodes': np.int64, 'v_nodes': np.int64,\n",
    "    #         'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    #     # use engine='python' to ignore warning about switching to python backend when using regexp for sep\n",
    "    #     data = pd.read_csv(filename, sep=sep, header=None,\n",
    "    #                        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], converters=dtypes, engine='python')\n",
    "\n",
    "    #     # shuffle here like cf-nade paper with python's own random class\n",
    "    #     # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "    #     data_array = data.values.tolist()\n",
    "    #     random.seed(seed)\n",
    "    #     random.shuffle(data_array)\n",
    "    #     data_array = np.array(data_array)\n",
    "\n",
    "    #     u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    #     v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    #     ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "    #     u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    #     v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    #     u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int64)\n",
    "    #     ratings = ratings.astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dataset name not recognized: ' + fname)\n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pINVydvo9IDs"
   },
   "source": [
    "Preprocessing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1aOz6674045b"
   },
   "outputs": [],
   "source": [
    "def normalize_features(feat):\n",
    "\n",
    "    degree = np.asarray(feat.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv = 1. / degree\n",
    "    degree_inv_mat = sp.diags([degree_inv], [0])\n",
    "    feat_norm = degree_inv_mat.dot(feat)\n",
    "\n",
    "    if feat_norm.nnz == 0:\n",
    "        print('ERROR: normalized adjacency matrix has only zero entries!!!!!')\n",
    "        exit\n",
    "\n",
    "    return feat_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJKhbrnE6IXX"
   },
   "outputs": [],
   "source": [
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file path\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir = np.asarray(ds['ir'])\n",
    "            jc = np.asarray(ds['jc'])\n",
    "            out = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xo3Bdpjr6jNS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeRijpxk6mXN"
   },
   "outputs": [],
   "source": [
    "def preprocess_user_item_features(u_features, v_features):\n",
    "    \"\"\"\n",
    "    Creates one big feature matrix out of user features and item features.\n",
    "    Stacks item features under the user features.\n",
    "    \"\"\"\n",
    "\n",
    "    zero_csr_u = sp.csr_matrix((u_features.shape[0], v_features.shape[1]), dtype=u_features.dtype)\n",
    "    zero_csr_v = sp.csr_matrix((v_features.shape[0], u_features.shape[1]), dtype=v_features.dtype)\n",
    "\n",
    "    u_features = sp.hstack([u_features, zero_csr_u], format='csr')\n",
    "    v_features = sp.hstack([zero_csr_v, v_features], format='csr')\n",
    "\n",
    "    return u_features, v_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggfej6Wm6mxP"
   },
   "outputs": [],
   "source": [
    "def globally_normalize_bipartite_adjacency(adjacencies, verbose=False, symmetric=True):\n",
    "    \"\"\" Globally Normalizes set of bipartite adjacency matrices \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('Symmetrically normalizing bipartite adj')\n",
    "    # degree_u and degree_v are row and column sums of adj+I\n",
    "\n",
    "    adj_tot = np.sum(adj for adj in adjacencies)\n",
    "    degree_u = np.asarray(adj_tot.sum(1)).flatten()\n",
    "    degree_v = np.asarray(adj_tot.sum(0)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree_u[degree_u == 0.] = np.inf\n",
    "    degree_v[degree_v == 0.] = np.inf\n",
    "\n",
    "    degree_u_inv_sqrt = 1. / np.sqrt(degree_u)\n",
    "    degree_v_inv_sqrt = 1. / np.sqrt(degree_v)\n",
    "    degree_u_inv_sqrt_mat = sp.diags([degree_u_inv_sqrt], [0])\n",
    "    degree_v_inv_sqrt_mat = sp.diags([degree_v_inv_sqrt], [0])\n",
    "\n",
    "    degree_u_inv = degree_u_inv_sqrt_mat.dot(degree_u_inv_sqrt_mat)\n",
    "\n",
    "    if symmetric:\n",
    "        adj_norm = [degree_u_inv_sqrt_mat.dot(adj).dot(degree_v_inv_sqrt_mat) for adj in adjacencies]\n",
    "\n",
    "    else:\n",
    "        adj_norm = [degree_u_inv.dot(adj) for adj in adjacencies]\n",
    "\n",
    "    return adj_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdhHjOO36r4f"
   },
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEvoe8rZ6ttq"
   },
   "outputs": [],
   "source": [
    "def create_trainvaltest_split(dataset, seed=1234, testing=False, datasplit_path=None, \n",
    "                              datasplit_from_file=False, verbose=True, rating_map=None, \n",
    "                              post_rating_map=None, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Splits data set into train/val/test sets from full bipartite adjacency matrix. Shuffling of dataset is done in\n",
    "    load_data function.\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if datasplit_from_file and os.path.isfile(datasplit_path):\n",
    "        print('Reading dataset splits from file...')\n",
    "        with open(datasplit_path, 'rb') as f:\n",
    "            num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pkl.load(f)\n",
    "\n",
    "        if verbose:\n",
    "            print('Number of users = %d' % num_users)\n",
    "            print('Number of items = %d' % num_items)\n",
    "            print('Number of links = %d' % ratings.shape[0])\n",
    "            print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    else:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed,\n",
    "                                                                                            verbose=verbose)\n",
    "\n",
    "        with open(datasplit_path, 'wb') as f:\n",
    "            pkl.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)\n",
    "\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    neutral_rating = -1\n",
    "\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "    num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "    if dataset == 'ml_100k':\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "    else:\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "\n",
    "    num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    train_idx = idx_nonzero[0:int(num_train*ratio)]\n",
    "    val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    train_pairs_idx = pairs_nonzero[0:int(num_train*ratio)]\n",
    "    val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U36FYgPC7SPb"
   },
   "outputs": [],
   "source": [
    "def load_official_trainvaltest_split(dataset, testing=False, rating_map=None, post_rating_map=None, ratio=1.0):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['u1.base', 'u1.test', 'u.item', 'u.user']\n",
    "    fname = dataset\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train ='u1.base'\n",
    "    filename_test = 'u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_array_train = data_train.values.tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.values.tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    if ratio < 1.0:\n",
    "        data_array_train = data_array_train[data_array_train[:, -1].argsort()[:int(ratio*len(data_array_train))]]\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "    if rating_map is not None:\n",
    "        for i, x in enumerate(ratings):\n",
    "            ratings[i] = rating_map[x]\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "    \n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    if post_rating_map is None:\n",
    "        rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    else:\n",
    "        rating_mx_train[train_idx] = np.array([post_rating_map[r] for r in class_values[labels[train_idx]]]) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    if dataset == 'ml_1m':\n",
    "\n",
    "        # load movie features\n",
    "        movies_file ='movies.dat'\n",
    "\n",
    "        movies_headers = ['movie_id', 'title', 'genre']\n",
    "        movies_df = pd.read_csv(movies_file, sep=sep, header=None,\n",
    "                                names=movies_headers, engine='python')\n",
    "\n",
    "        # extracting all genres\n",
    "        genres = []\n",
    "        for s in movies_df['genre'].values:\n",
    "            genres.extend(s.split('|'))\n",
    "\n",
    "        genres = list(set(genres))\n",
    "        num_genres = len(genres)\n",
    "\n",
    "        genres_dict = {g: idx for idx, g in enumerate(genres)}\n",
    "\n",
    "        # creating 0 or 1 valued features for all genres\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, s in zip(movies_df['movie_id'].values.tolist(), movies_df['genre'].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                gen = s.split('|')\n",
    "                for g in gen:\n",
    "                    v_features[v_dict[movie_id], genres_dict[g]] = 1.\n",
    "\n",
    "        # load user features\n",
    "        users_file = 'users.dat'\n",
    "        users_headers = ['user_id', 'gender', 'age', 'occupation', 'zip-code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        # extracting all features\n",
    "        cols = users_df.columns.values[1:]\n",
    "\n",
    "        cntr = 0\n",
    "        feat_dicts = []\n",
    "        for header in cols:\n",
    "            d = dict()\n",
    "            feats = np.unique(users_df[header].values).tolist()\n",
    "            d.update({f: i for i, f in enumerate(feats, start=cntr)})\n",
    "            feat_dicts.append(d)\n",
    "            cntr += len(d)\n",
    "\n",
    "        num_feats = sum(len(d) for d in feat_dicts)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user_id']\n",
    "            if u_id in u_dict.keys():\n",
    "                for k, header in enumerate(cols):\n",
    "                    u_features[u_dict[u_id], feat_dicts[k][row[header]]] = 1.\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xtm5Qgae9ySJ"
   },
   "source": [
    "util_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "nARMk_7K8fGE",
    "outputId": "b3b5673f-7720-42bc-a680-26093c23890e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6aa2d34e28f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseEfficiencyWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os, sys, pdb, math, time\n",
    "from copy import deepcopy\n",
    "import multiprocessing as mp\n",
    "import networkx as nx\n",
    "import argparse\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as ssp\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, InMemoryDataset\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', ssp.SparseEfficiencyWarning)\n",
    "cur_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "DWSj0SN890-A",
    "outputId": "30964dfc-efb3-449a-f5e5-e8bfc58332ae"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9d68e03a7b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyDynamicDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, root, A, links, labels, h, sample_ratio, max_nodes_per_hop, \n\u001b[1;32m      3\u001b[0m                  u_features, v_features, max_node_label, class_values):\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyDynamicDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class MyDynamicDataset(Dataset):\n",
    "    def __init__(self, root, A, links, labels, h, sample_ratio, max_nodes_per_hop, \n",
    "                 u_features, v_features, max_node_label, class_values):\n",
    "        super(MyDynamicDataset, self).__init__(root)\n",
    "        self.A = A\n",
    "        self.links = links\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.max_nodes_per_hop = max_nodes_per_hop\n",
    "        self.u_features = u_features\n",
    "        self.v_features = v_features\n",
    "        self.max_node_label = max_node_label\n",
    "        self.class_values = class_values\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def _download(self):\n",
    "        pass\n",
    "\n",
    "    def _process(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.links[0])\n",
    "\n",
    "    def get(self, idx):\n",
    "        i, j = self.links[0][idx], self.links[1][idx]\n",
    "        g, n_labels, n_features = subgraph_extraction_labeling(\n",
    "            (i, j), self.A, self.h, self.sample_ratio, self.max_nodes_per_hop, \n",
    "            self.u_features, self.v_features, self.class_values\n",
    "        )\n",
    "        g_label = self.labels[idx]\n",
    "        return nx_to_PyGGraph(\n",
    "            g, g_label, n_labels, n_features, self.max_node_label, self.class_values\n",
    "        )\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "-VHafnZG99MR",
    "outputId": "16896c98-8624-4608-c462-710f88fc9c71"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9d68e03a7b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyDynamicDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, root, A, links, labels, h, sample_ratio, max_nodes_per_hop, \n\u001b[1;32m      3\u001b[0m                  u_features, v_features, max_node_label, class_values):\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyDynamicDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class MyDynamicDataset(Dataset):\n",
    "    def __init__(self, root, A, links, labels, h, sample_ratio, max_nodes_per_hop, \n",
    "                 u_features, v_features, max_node_label, class_values):\n",
    "        super(MyDynamicDataset, self).__init__(root)\n",
    "        self.A = A\n",
    "        self.links = links\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.max_nodes_per_hop = max_nodes_per_hop\n",
    "        self.u_features = u_features\n",
    "        self.v_features = v_features\n",
    "        self.max_node_label = max_node_label\n",
    "        self.class_values = class_values\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def _download(self):\n",
    "        pass\n",
    "\n",
    "    def _process(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.links[0])\n",
    "\n",
    "    def get(self, idx):\n",
    "        i, j = self.links[0][idx], self.links[1][idx]\n",
    "        g, n_labels, n_features = subgraph_extraction_labeling(\n",
    "            (i, j), self.A, self.h, self.sample_ratio, self.max_nodes_per_hop, \n",
    "            self.u_features, self.v_features, self.class_values\n",
    "        )\n",
    "        g_label = self.labels[idx]\n",
    "        return nx_to_PyGGraph(\n",
    "            g, g_label, n_labels, n_features, self.max_node_label, self.class_values\n",
    "        )\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbOM8409-I3f"
   },
   "outputs": [],
   "source": [
    "def PyGGraph_to_nx(data):\n",
    "    edges = list(zip(data.edge_index[0, :].tolist(), data.edge_index[1, :].tolist()))\n",
    "    g = nx.from_edgelist(edges)\n",
    "    g.add_nodes_from(range(len(data.x)))  # in case some nodes are isolated\n",
    "    # transform r back to rating label\n",
    "    edge_types = {(u, v): data.edge_type[i].item() for i, (u, v) in enumerate(edges)}  \n",
    "    nx.set_edge_attributes(g, name='type', values=edge_types)\n",
    "    node_types = dict(zip(range(data.num_nodes), torch.argmax(data.x, 1).tolist()))\n",
    "    nx.set_node_attributes(g, name='type', values=node_types)\n",
    "    g.graph['rating'] = data.y.item()\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEHqCpi0-LUf"
   },
   "outputs": [],
   "source": [
    "def links2subgraphs(A,\n",
    "                    train_indices, \n",
    "                    val_indices, \n",
    "                    test_indices, \n",
    "                    train_labels, \n",
    "                    val_labels, \n",
    "                    test_labels, \n",
    "                    h=1, \n",
    "                    sample_ratio=1.0, \n",
    "                    max_nodes_per_hop=None, \n",
    "                    u_features=None, \n",
    "                    v_features=None, \n",
    "                    max_node_label=None, \n",
    "                    class_values=None, \n",
    "                    testing=False, \n",
    "                    parallel=True):\n",
    "    # extract enclosing subgraphs\n",
    "    if max_node_label is None:  # if not provided, infer from graphs\n",
    "        max_n_label = {'max_node_label': 0}\n",
    "\n",
    "    def helper(A, links, g_labels):\n",
    "        g_list = []\n",
    "        if not parallel or max_node_label is None:\n",
    "            with tqdm(total=len(links[0])) as pbar:\n",
    "                for i, j, g_label in zip(links[0], links[1], g_labels):\n",
    "                    g, n_labels, n_features = subgraph_extraction_labeling(\n",
    "                        (i, j), A, h, sample_ratio, max_nodes_per_hop, u_features, \n",
    "                        v_features, class_values\n",
    "                    )\n",
    "                    if max_node_label is None:\n",
    "                        max_n_label['max_node_label'] = max(\n",
    "                            max(n_labels), max_n_label['max_node_label']\n",
    "                        )\n",
    "                        g_list.append((g, g_label, n_labels, n_features))\n",
    "                    else:\n",
    "                        g_list.append(nx_to_PyGGraph(\n",
    "                            g, g_label, n_labels, n_features, max_node_label, class_values\n",
    "                        ))\n",
    "                    pbar.update(1)\n",
    "        else:\n",
    "            start = time.time()\n",
    "            pool = mp.Pool(mp.cpu_count())\n",
    "            results = pool.starmap_async(\n",
    "                parallel_worker, \n",
    "                [\n",
    "                    (g_label, (i, j), A, h, sample_ratio, max_nodes_per_hop, u_features, \n",
    "                    v_features, class_values) \n",
    "                    for i, j, g_label in zip(links[0], links[1], g_labels)\n",
    "                ]\n",
    "            )\n",
    "            remaining = results._number_left\n",
    "            pbar = tqdm(total=remaining)\n",
    "            while True:\n",
    "                pbar.update(remaining - results._number_left)\n",
    "                if results.ready(): break\n",
    "                remaining = results._number_left\n",
    "                time.sleep(1)\n",
    "            results = results.get()\n",
    "            pool.close()\n",
    "            pbar.close()\n",
    "            end = time.time()\n",
    "            print(\"Time eplased for subgraph extraction: {}s\".format(end-start))\n",
    "            print(\"Transforming to pytorch_geometric graphs...\".format(end-start))\n",
    "            g_list += [\n",
    "                nx_to_PyGGraph(g, g_label, n_labels, n_features, max_node_label, class_values) \n",
    "                for g_label, g, n_labels, n_features in tqdm(results)\n",
    "            ]\n",
    "            del results\n",
    "            end2 = time.time()\n",
    "            print(\"Time eplased for transforming to pytorch_geometric graphs: {}s\".format(end2-end))\n",
    "        return g_list\n",
    "\n",
    "    print('Enclosing subgraph extraction begins...')\n",
    "    train_graphs = helper(A, train_indices, train_labels)\n",
    "    if not testing:\n",
    "        val_graphs = helper(A, val_indices, val_labels)\n",
    "    else:\n",
    "        val_graphs = []\n",
    "    test_graphs = helper(A, test_indices, test_labels)\n",
    "\n",
    "    if max_node_label is None:\n",
    "        train_graphs = [\n",
    "            nx_to_PyGGraph(*x, **max_n_label, class_values=class_values) for x in train_graphs\n",
    "        ]\n",
    "        val_graphs = [\n",
    "            nx_to_PyGGraph(*x, **max_n_label, class_values=class_values) for x in val_graphs\n",
    "        ]\n",
    "        test_graphs = [\n",
    "            nx_to_PyGGraph(*x, **max_n_label, class_values=class_values) for x in test_graphs\n",
    "        ]\n",
    "    \n",
    "    return train_graphs, val_graphs, test_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "to1BGB30-QIh"
   },
   "outputs": [],
   "source": [
    "def subgraph_extraction_labeling(ind, A, h=1, sample_ratio=1.0, max_nodes_per_hop=None, \n",
    "                                 u_features=None, v_features=None, class_values=None):\n",
    "    # extract the h-hop enclosing subgraph around link 'ind'\n",
    "    dist = 0\n",
    "    u_nodes, v_nodes = [ind[0]], [ind[1]]\n",
    "    u_dist, v_dist = [0], [0]\n",
    "    u_visited, v_visited = set([ind[0]]), set([ind[1]])\n",
    "    u_fringe, v_fringe = set([ind[0]]), set([ind[1]])\n",
    "    for dist in range(1, h+1):\n",
    "        v_fringe, u_fringe = neighbors(u_fringe, A, True), neighbors(v_fringe, A, False)\n",
    "        u_fringe = u_fringe - u_visited\n",
    "        v_fringe = v_fringe - v_visited\n",
    "        u_visited = u_visited.union(u_fringe)\n",
    "        v_visited = v_visited.union(v_fringe)\n",
    "        if sample_ratio < 1.0:\n",
    "            u_fringe = random.sample(u_fringe, int(sample_ratio*len(u_fringe)))\n",
    "            v_fringe = random.sample(v_fringe, int(sample_ratio*len(v_fringe)))\n",
    "        if max_nodes_per_hop is not None:\n",
    "            if max_nodes_per_hop < len(u_fringe):\n",
    "                u_fringe = random.sample(u_fringe, max_nodes_per_hop)\n",
    "            if max_nodes_per_hop < len(v_fringe):\n",
    "                v_fringe = random.sample(v_fringe, max_nodes_per_hop)\n",
    "        if len(u_fringe) == 0 and len(v_fringe) == 0:\n",
    "            break\n",
    "        u_nodes = u_nodes + list(u_fringe)\n",
    "        v_nodes = v_nodes + list(v_fringe)\n",
    "        u_dist = u_dist + [dist] * len(u_fringe)\n",
    "        v_dist = v_dist + [dist] * len(v_fringe)\n",
    "    subgraph = A[u_nodes, :][:, v_nodes]\n",
    "    # remove link between target nodes\n",
    "    subgraph[0, 0] = 0\n",
    "    # construct nx graph\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(range(len(u_nodes)), bipartite='u')\n",
    "    g.add_nodes_from(range(len(u_nodes), len(u_nodes)+len(v_nodes)), bipartite='v')\n",
    "    u, v, r = ssp.find(subgraph)  # r is 1, 2... (rating labels + 1)\n",
    "    r = r.astype(int)\n",
    "    v += len(u_nodes)\n",
    "    #g.add_weighted_edges_from(zip(u, v, r))\n",
    "    g.add_edges_from(zip(u, v))\n",
    "\n",
    "    edge_types = dict(zip(zip(u, v), r-1))  # transform r back to rating label\n",
    "    nx.set_edge_attributes(g, name='type', values=edge_types)\n",
    "    # get structural node labels\n",
    "    node_labels = [x*2 for x in u_dist] + [x*2+1 for x in v_dist]\n",
    "\n",
    "    # get node features\n",
    "    if u_features is not None:\n",
    "        u_features = u_features[u_nodes]\n",
    "    if v_features is not None:\n",
    "        v_features = v_features[v_nodes]\n",
    "    node_features = None\n",
    "    if False: \n",
    "        # directly use padded node features\n",
    "        if u_features is not None and v_features is not None:\n",
    "            u_extended = np.concatenate(\n",
    "                [u_features, np.zeros([u_features.shape[0], v_features.shape[1]])], 1\n",
    "            )\n",
    "            v_extended = np.concatenate(\n",
    "                [np.zeros([v_features.shape[0], u_features.shape[1]]), v_features], 1\n",
    "            )\n",
    "            node_features = np.concatenate([u_extended, v_extended], 0)\n",
    "    if False:\n",
    "        # use identity features (one-hot encodings of node idxes)\n",
    "        u_ids = one_hot(u_nodes, A.shape[0]+A.shape[1])\n",
    "        v_ids = one_hot([x+A.shape[0] for x in v_nodes], A.shape[0]+A.shape[1])\n",
    "        node_ids = np.concatenate([u_ids, v_ids], 0)\n",
    "        #node_features = np.concatenate([node_features, node_ids], 1)\n",
    "        node_features = node_ids\n",
    "    if True:\n",
    "        # only output node features for the target user and item\n",
    "        if u_features is not None and v_features is not None:\n",
    "            node_features = [u_features[0], v_features[0]]\n",
    "\n",
    "    return g, node_labels, node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcfGSKah-TpD"
   },
   "outputs": [],
   "source": [
    "def parallel_worker(g_label, ind, A, h=1, sample_ratio=1.0, max_nodes_per_hop=None, \n",
    "                    u_features=None, v_features=None, class_values=None):\n",
    "    g, node_labels, node_features = subgraph_extraction_labeling(\n",
    "        ind, A, h, sample_ratio, max_nodes_per_hop, u_features, v_features, class_values\n",
    "    )\n",
    "    return g_label, g, node_labels, node_features\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cc5MYWl4-VnA"
   },
   "outputs": [],
   "source": [
    "def neighbors(fringe, A, row=True):\n",
    "    # find all 1-hop neighbors of nodes in fringe from A\n",
    "    res = set()\n",
    "    for node in fringe:\n",
    "        if row:\n",
    "            _, nei, _ = ssp.find(A[node, :])\n",
    "        else:\n",
    "            nei, _, _ = ssp.find(A[:, node])\n",
    "        nei = set(nei)\n",
    "        res = res.union(nei)\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmVvZQTS-XOG"
   },
   "outputs": [],
   "source": [
    "def one_hot(idx, length):\n",
    "    idx = np.array(idx)\n",
    "    x = np.zeros([len(idx), length])\n",
    "    x[np.arange(len(idx)), idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDVhcwHg-aUF"
   },
   "source": [
    "models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "ZdKz9xqo-gJA",
    "outputId": "8a3d53c3-2e5a-4021-b109-50fd972b74fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/f2/26359fb7b50d54924ddd23778d4830b2653df9ffe72f85caad2b829dc778/torch_geometric-1.5.0.tar.gz (153kB)\n",
      "\r",
      "\u001b[K     |                             | 10kB 13.2MB/s eta 0:00:01\r",
      "\u001b[K     |                           | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |                         | 30kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |                       | 40kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |                     | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |                   | 61kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |                 | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |               | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |            | 92kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |          | 102kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |        | 112kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |      | 122kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |    | 133kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |  | 143kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     || 153kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.5.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (4.41.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.22.2.post1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.48.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.23.0)\n",
      "Collecting plyfile\n",
      "  Downloading https://files.pythonhosted.org/packages/93/c8/cf47848cd4d661850e4a8e7f0fc4f7298515e06d0da7255ed08e5312d4aa/plyfile-0.7.2-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (1.0.4)\n",
      "Collecting rdflib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
      "\u001b[K     || 235kB 12.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (2.10.0)\n",
      "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch_geometric) (0.4)\n",
      "Collecting ase\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     || 2.1MB 12.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch_geometric) (0.16.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch_geometric) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch_geometric) (0.15.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (47.1.1)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch_geometric) (0.31.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch_geometric) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch_geometric) (2.8.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch_geometric) (1.12.0)\n",
      "Collecting isodate\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "\u001b[K     || 51kB 5.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch_geometric) (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch_geometric) (0.10.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-1.5.0-cp36-none-any.whl size=267918 sha256=5e3ebcb2817f241b39d63df487ca49de5ed9131dc754033417d063ea23d006ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/ec/51/31/5786f2ac419ee312f22d4d2877da05f20e7f2d430e22917daf\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: plyfile, isodate, rdflib, ase, torch-geometric\n",
      "Successfully installed ase-3.19.1 isodate-0.6.0 plyfile-0.7.2 rdflib-5.0.0 torch-geometric-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "w7faFDMx-ycr",
    "outputId": "b8d455c8-db71-48e9-9416-d608d3e814a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
      "Collecting torch-sparse==latest+cu101\n",
      "  Using cached https://pytorch-geometric.com/whl/torch-1.4.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
      "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.0+cu101)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
      "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (3.19.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
      "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (47.1.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.15.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
      "Installing collected packages: torch-sparse\n",
      "  Found existing installation: torch-sparse 0.6.1\n",
      "    Uninstalling torch-sparse-0.6.1:\n",
      "      Successfully uninstalled torch-sparse-0.6.1\n",
      "Successfully installed torch-sparse-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric \\\n",
    "  torch-sparse==latest+cu101 \\\n",
    "  -f https://pytorch-geometric.com/whl/torch-1.4.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "D9IiPYag-nBE",
    "outputId": "96adffa6-a6c1-437b-fa96-7364311ed604"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fe816a438956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_sort_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_add_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdropout_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0min_memory_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataListLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDenseDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from torch_geometric.utils import (contains_isolated_nodes,\n\u001b[1;32m      9\u001b[0m                                    contains_self_loops, is_undirected)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt_minor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mminor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;34mf'Detected that PyTorch and torch_sparse were compiled with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;34mf'different CUDA versions. PyTorch has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34mf'{t_major}.{t_minor} and torch_sparse has CUDA version '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torch_sparse were compiled with different CUDA versions. PyTorch has CUDA version 10.1 and torch_sparse has CUDA version 0.0. Please reinstall the torch_sparse that matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Conv1d\n",
    "from torch_geometric.nn import GCNConv, RGCNConv, global_sort_pool, global_add_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from util_functions import *\n",
    "import pdb\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "eNJWjhsS-tbR",
    "outputId": "3bbc157b-8a71-4f3f-a910-b4d3b8492d55"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-60ecd190c60d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# a base GNN class, GCN message passing + sum_pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, dataset, gconv=GCNConv, latent_dim=[32, 32, 32, 1], \n\u001b[1;32m      4\u001b[0m                  regression=False, adj_dropout=0.2, force_undirected=False):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-60ecd190c60d>\u001b[0m in \u001b[0;36mGNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# a base GNN class, GCN message passing + sum_pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     def __init__(self, dataset, gconv=GCNConv, latent_dim=[32, 32, 32, 1], \n\u001b[0m\u001b[1;32m      4\u001b[0m                  regression=False, adj_dropout=0.2, force_undirected=False):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GCNConv' is not defined"
     ]
    }
   ],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    # a base GNN class, GCN message passing + sum_pooling\n",
    "    def __init__(self, dataset, gconv=GCNConv, latent_dim=[32, 32, 32, 1], \n",
    "                 regression=False, adj_dropout=0.2, force_undirected=False):\n",
    "        super(GNN, self).__init__()\n",
    "        self.regression = regression\n",
    "        self.adj_dropout = adj_dropout \n",
    "        self.force_undirected = force_undirected\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(gconv(dataset.num_features, latent_dim[0]))\n",
    "        for i in range(0, len(latent_dim)-1):\n",
    "            self.convs.append(gconv(latent_dim[i], latent_dim[i+1]))\n",
    "        self.lin1 = Linear(sum(latent_dim), 128)\n",
    "        if self.regression:\n",
    "            self.lin2 = Linear(128, 1)\n",
    "        else:\n",
    "            self.lin2 = Linear(128, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if self.adj_dropout > 0:\n",
    "            edge_index, edge_type = dropout_adj(\n",
    "                edge_index, edge_type, p=self.adj_dropout, \n",
    "                force_undirected=self.force_undirected, num_nodes=len(x), \n",
    "                training=self.training\n",
    "            )\n",
    "        concat_states = []\n",
    "        for conv in self.convs:\n",
    "            x = torch.tanh(conv(x, edge_index))\n",
    "            concat_states.append(x)\n",
    "        concat_states = torch.cat(concat_states, 1)\n",
    "        x = global_add_pool(concat_states, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        if self.regression:\n",
    "            return x[:, 0]\n",
    "        else:\n",
    "            return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4v3bSTqX_Mg1"
   },
   "outputs": [],
   "source": [
    "class IGMC(GNN):\n",
    "    # The GNN model of Inductive Graph-based Matrix Completion. \n",
    "    # Use RGCN convolution + center-nodes readout.\n",
    "    def __init__(self, dataset, gconv=RGCNConv, latent_dim=[32, 32, 32, 32], \n",
    "                 num_relations=5, num_bases=2, regression=False, adj_dropout=0.2, \n",
    "                 force_undirected=False, side_features=False, n_side_features=0, \n",
    "                 multiply_by=1):\n",
    "        super(IGMC, self).__init__(\n",
    "            dataset, GCNConv, latent_dim, regression, adj_dropout, force_undirected\n",
    "        )\n",
    "        self.multiply_by = multiply_by\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(gconv(dataset.num_features, latent_dim[0], num_relations, num_bases))\n",
    "        for i in range(0, len(latent_dim)-1):\n",
    "            self.convs.append(gconv(latent_dim[i], latent_dim[i+1], num_relations, num_bases))\n",
    "        self.lin1 = Linear(2*sum(latent_dim), 128)\n",
    "        self.side_features = side_features\n",
    "        if side_features:\n",
    "            self.lin1 = Linear(2*sum(latent_dim)+n_side_features, 128)\n",
    "\n",
    "    def forward(self, data):\n",
    "        start = time.time()\n",
    "        x, edge_index, edge_type, batch = data.x, data.edge_index, data.edge_type, data.batch\n",
    "        if self.adj_dropout > 0:\n",
    "            edge_index, edge_type = dropout_adj(\n",
    "                edge_index, edge_type, p=self.adj_dropout, \n",
    "                force_undirected=self.force_undirected, num_nodes=len(x), \n",
    "                training=self.training\n",
    "            )\n",
    "        concat_states = []\n",
    "        for conv in self.convs:\n",
    "            x = torch.tanh(conv(x, edge_index, edge_type))\n",
    "            concat_states.append(x)\n",
    "        concat_states = torch.cat(concat_states, 1)\n",
    "\n",
    "        users = data.x[:, 0] == 1\n",
    "        items = data.x[:, 1] == 1\n",
    "        x = torch.cat([concat_states[users], concat_states[items]], 1)\n",
    "        if self.side_features:\n",
    "            x = torch.cat([x, data.u_feature, data.v_feature], 1)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        if self.regression:\n",
    "            return x[:, 0] * self.multiply_by\n",
    "        else:\n",
    "            return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMXtTttz_ZIM"
   },
   "source": [
    "train_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQZpfMsd_RU7"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from util_functions import PyGGraph_to_nx\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i2hSHnx_cfn"
   },
   "outputs": [],
   "source": [
    "def train_multiple_epochs(train_dataset,\n",
    "                          test_dataset,\n",
    "                          model,\n",
    "                          epochs,\n",
    "                          batch_size,\n",
    "                          lr,\n",
    "                          lr_decay_factor,\n",
    "                          lr_decay_step_size,\n",
    "                          weight_decay,\n",
    "                          ARR=0, \n",
    "                          logger=None, \n",
    "                          continue_from=None, \n",
    "                          res_dir=None):\n",
    "\n",
    "    rmses = []\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=mp.cpu_count())\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "    model.to(device).reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    start_epoch = 1\n",
    "    if continue_from is not None:\n",
    "        model.load_state_dict(\n",
    "            torch.load(os.path.join(res_dir, 'model_checkpoint{}.pth'.format(continue_from)))\n",
    "        )\n",
    "        optimizer.load_state_dict(\n",
    "            torch.load(os.path.join(res_dir, 'optimizer_checkpoint{}.pth'.format(continue_from)))\n",
    "        )\n",
    "        start_epoch = continue_from + 1\n",
    "        epochs -= continue_from\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_start = time.perf_counter()\n",
    "    pbar = tqdm(range(start_epoch, epochs + start_epoch))\n",
    "    for epoch in pbar:\n",
    "        train_loss = train(model, optimizer, train_loader, device, regression=True, ARR=ARR)\n",
    "        rmses.append(eval_rmse(model, test_loader, device))\n",
    "        eval_info = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'test_rmse': rmses[-1],\n",
    "        }\n",
    "        pbar.set_description(\n",
    "            'Epoch {}, train loss {:.6f}, test rmse {:.6f}'.format(*eval_info.values())\n",
    "        )\n",
    "\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        if logger is not None:\n",
    "            logger(eval_info, model, optimizer)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t_end = time.perf_counter()\n",
    "    duration = t_end - t_start\n",
    "\n",
    "    print('Final Test RMSE: {:.6f}, Duration: {:.6f}'.\n",
    "          format(rmses[-1],\n",
    "                 duration))\n",
    "\n",
    "    return rmses[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7qK4q78_hRQ"
   },
   "outputs": [],
   "source": [
    "def test_once(test_dataset,\n",
    "              model,\n",
    "              batch_size,\n",
    "              logger=None, \n",
    "              ensemble=False, \n",
    "              checkpoints=None):\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    model.to(device)\n",
    "    t_start = time.perf_counter()\n",
    "    if ensemble and checkpoints:\n",
    "        rmse = eval_rmse_ensemble(model, checkpoints, test_loader, device, show_progress=True)\n",
    "    else:\n",
    "        rmse = eval_rmse(model, test_loader, device, show_progress=True)\n",
    "    t_end = time.perf_counter()\n",
    "    duration = t_end - t_start\n",
    "    print('Test Once RMSE: {:.6f}, Duration: {:.6f}'.format(rmse, duration))\n",
    "    epoch_info = 'test_once' if not ensemble else 'ensemble'\n",
    "    eval_info = {\n",
    "        'epoch': epoch_info,\n",
    "        'train_loss': 0,\n",
    "        'test_rmse': rmse,\n",
    "        }\n",
    "    if logger is not None:\n",
    "        logger(eval_info, None, None)\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXUPgffx_j2a"
   },
   "outputs": [],
   "source": [
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD0gPRkP_mC9"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, device, regression=False, ARR=0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        if regression:\n",
    "            loss = F.mse_loss(out, data.y.view(-1))\n",
    "        else:\n",
    "            loss = F.nll_loss(out, data.y.view(-1))\n",
    "        if ARR != 0:\n",
    "            for gconv in model.convs:\n",
    "                w = torch.matmul(\n",
    "                    gconv.att, \n",
    "                    gconv.basis.view(gconv.num_bases, -1)\n",
    "                ).view(gconv.num_relations, gconv.in_channels, gconv.out_channels)\n",
    "                reg_loss = torch.sum((w[1:, :, :] - w[:-1, :, :])**2)\n",
    "                loss += ARR * reg_loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRg5FDqJ_n-p"
   },
   "outputs": [],
   "source": [
    "def eval_loss(model, loader, device, regression=False, show_progress=False):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    if show_progress:\n",
    "        print('Testing begins...')\n",
    "        pbar = tqdm(loader)\n",
    "    else:\n",
    "        pbar = loader\n",
    "    for data in pbar:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        if regression:\n",
    "            loss += F.mse_loss(out, data.y.view(-1), reduction='sum').item()\n",
    "        else:\n",
    "            loss += F.nll_loss(out, data.y.view(-1), reduction='sum').item()\n",
    "        torch.cuda.empty_cache()\n",
    "    return loss / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvz0ZNIj_qDX"
   },
   "outputs": [],
   "source": [
    "def eval_rmse(model, loader, device, show_progress=False):\n",
    "    mse_loss = eval_loss(model, loader, device, True, show_progress)\n",
    "    rmse = math.sqrt(mse_loss)\n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7cnO9hA_r4Z"
   },
   "outputs": [],
   "source": [
    "def eval_loss_ensemble(model, checkpoints, loader, device, regression=False, show_progress=False):\n",
    "    loss = 0\n",
    "    Outs = []\n",
    "    for i, checkpoint in enumerate(checkpoints):\n",
    "        if show_progress:\n",
    "            print('Testing begins...')\n",
    "            pbar = tqdm(loader)\n",
    "        else:\n",
    "            pbar = loader\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        if i == 0:\n",
    "            ys = []\n",
    "        for data in pbar:\n",
    "            data = data.to(device)\n",
    "            if i == 0:\n",
    "                ys.append(data.y.view(-1))\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                outs.append(out)\n",
    "        if i == 0:\n",
    "            ys = torch.cat(ys, 0)\n",
    "        outs = torch.cat(outs, 0).view(-1, 1)\n",
    "        Outs.append(outs)\n",
    "    Outs = torch.cat(Outs, 1).mean(1)\n",
    "    if regression:\n",
    "        loss += F.mse_loss(Outs, ys, reduction='sum').item()\n",
    "    else:\n",
    "        loss += F.nll_loss(Outs, ys, reduction='sum').item()\n",
    "    torch.cuda.empty_cache()\n",
    "    return loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKkgglST_utw"
   },
   "outputs": [],
   "source": [
    "def eval_rmse_ensemble(model, checkpoints, loader, device, show_progress=False):\n",
    "    mse_loss = eval_loss_ensemble(model, checkpoints, loader, device, True, show_progress)\n",
    "    rmse = math.sqrt(mse_loss)\n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvmJTccN_wpE"
   },
   "outputs": [],
   "source": [
    "def visualize(model, graphs, res_dir, data_name, class_values, num=5, sort_by='prediction'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    R = []\n",
    "    Y = []\n",
    "    graph_loader = DataLoader(graphs, 50, shuffle=False)\n",
    "    for data in tqdm(graph_loader):\n",
    "        data = data.to(device)\n",
    "        r = model(data).detach()\n",
    "        y = data.y\n",
    "        R.extend(r.view(-1).tolist())\n",
    "        Y.extend(y.view(-1).tolist())\n",
    "    if sort_by == 'true':  # sort graphs by their true ratings\n",
    "        order = np.argsort(Y).tolist()\n",
    "    elif sort_by == 'prediction':\n",
    "        order = np.argsort(R).tolist()\n",
    "    elif sort_by == 'random':  # randomly select graphs to visualize\n",
    "        order = np.random.permutation(range(len(R))).tolist()\n",
    "    highest = [PyGGraph_to_nx(graphs[i]) for i in order[-num:][::-1]]\n",
    "    lowest = [PyGGraph_to_nx(graphs[i]) for i in order[:num]]\n",
    "    highest_scores = [R[i] for i in order[-num:][::-1]]\n",
    "    lowest_scores = [R[i] for i in order[:num]]\n",
    "    highest_ys = [Y[i] for i in order[-num:][::-1]]\n",
    "    lowest_ys = [Y[i] for i in order[:num]]\n",
    "    scores = highest_scores + lowest_scores\n",
    "    ys = highest_ys + lowest_ys\n",
    "    type_to_label = {0: 'u0', 1: 'v0', 2: 'u1', 3: 'v1', 4: 'u2', 5: 'v2'}\n",
    "    type_to_color = {0: 'xkcd:red', 1: 'xkcd:blue', 2: 'xkcd:orange', \n",
    "                     3: 'xkcd:lightblue', 4: 'y', 5: 'g'}\n",
    "    plt.axis('off')\n",
    "    f = plt.figure(figsize=(20, 10))\n",
    "    axs = f.subplots(2, num)\n",
    "    cmap = plt.cm.get_cmap('rainbow')\n",
    "    vmin, vmax = min(class_values), max(class_values)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    for i, g in enumerate(highest + lowest):\n",
    "        u_nodes = [x for x, y in g.nodes(data=True) if y['type'] % 2 == 0]\n",
    "        u0, v0 = 0, len(u_nodes)\n",
    "        pos = nx.drawing.layout.bipartite_layout(g, u_nodes)\n",
    "        bottom_u_node = min(pos, key=lambda x: (pos[x][0], pos[x][1]))\n",
    "        bottom_v_node = min(pos, key=lambda x: (-pos[x][0], pos[x][1]))\n",
    "        # swap u0 and v0 with bottom nodes if they are not already\n",
    "        if u0 != bottom_u_node:\n",
    "            pos[u0], pos[bottom_u_node] = pos[bottom_u_node], pos[u0]\n",
    "        if v0 != bottom_v_node:\n",
    "            pos[v0], pos[bottom_v_node] = pos[bottom_v_node], pos[v0]\n",
    "        labels = {x: type_to_label[y] for x, y in nx.get_node_attributes(g, 'type').items()}\n",
    "        node_colors = [type_to_color[y] for x, y in nx.get_node_attributes(g, 'type').items()]\n",
    "        edge_types = nx.get_edge_attributes(g, 'type')\n",
    "        edge_types = [class_values[edge_types[x]] for x in g.edges()]\n",
    "        axs[i//num, i%num].axis('off')\n",
    "        nx.draw_networkx(g, pos, \n",
    "                #labels=labels, \n",
    "                with_labels=False, \n",
    "                node_size=150, \n",
    "                node_color=node_colors, edge_color=edge_types, \n",
    "                ax=axs[i//num, i%num], edge_cmap=cmap, edge_vmin=vmin, edge_vmax=vmax, \n",
    "                )\n",
    "        # make u0 v0 on top of other nodes\n",
    "        nx.draw_networkx_nodes(g, {u0: pos[u0]}, nodelist=[u0], node_size=150,\n",
    "                node_color='xkcd:red', ax=axs[i//num, i%num])\n",
    "        nx.draw_networkx_nodes(g, {v0: pos[v0]}, nodelist=[v0], node_size=150,\n",
    "                node_color='xkcd:blue', ax=axs[i//num, i%num])\n",
    "        axs[i//num, i%num].set_title('{:.4f} ({:})'.format(\n",
    "            scores[i], ys[i]), x=0.5, y=-0.05, fontsize=20\n",
    "        )\n",
    "    f.subplots_adjust(right=0.85)\n",
    "    cbar_ax = f.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "    if len(class_values) > 20:\n",
    "        class_values = np.linspace(min(class_values), max(class_values), 20, dtype=int).tolist()\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax, ticks=class_values)\n",
    "    cbar.ax.tick_params(labelsize=22)\n",
    "    f.savefig(os.path.join(res_dir, \"visualization_{}_{}.pdf\".format(data_name, sort_by)), \n",
    "            interpolation='nearest', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYwZ2r-h_2NZ"
   },
   "source": [
    "MAIN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WXrYEZ0_1xv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys, copy, math, time, pdb, warnings, traceback\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as ssp\n",
    "import os.path\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copy, rmtree, copytree\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from util_functions import *\n",
    "from data_utils import *\n",
    "from preprocessing import *\n",
    "from train_eval import *\n",
    "from models import *\n",
    "\n",
    "import traceback\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# used to traceback which code cause warnings, can delete\n",
    "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
    "\n",
    "    log = file if hasattr(file,'write') else sys.stderr\n",
    "    traceback.print_stack(file=log)\n",
    "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "warnings.showwarning = warn_with_traceback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KyFwmqx_1Kj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IGMC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
