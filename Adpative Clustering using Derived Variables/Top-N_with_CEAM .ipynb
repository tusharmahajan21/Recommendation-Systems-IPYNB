{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import math \n",
    "import random\n",
    "import time as t\n",
    "\n",
    "from pandas import DataFrame, merge\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.modela_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>avg_rating_customer</th>\n",
       "      <th>quantity_customer</th>\n",
       "      <th>seen_popularity</th>\n",
       "      <th>seen_rating</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>20</td>\n",
       "      <td>45.550000</td>\n",
       "      <td>3.562529</td>\n",
       "      <td>female</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.486842</td>\n",
       "      <td>3.486842</td>\n",
       "      <td>76</td>\n",
       "      <td>106.578947</td>\n",
       "      <td>3.536646</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.568627</td>\n",
       "      <td>3.568627</td>\n",
       "      <td>51</td>\n",
       "      <td>116.843137</td>\n",
       "      <td>3.716531</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.348039</td>\n",
       "      <td>4.348039</td>\n",
       "      <td>204</td>\n",
       "      <td>72.480392</td>\n",
       "      <td>3.610074</td>\n",
       "      <td>female</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.910000</td>\n",
       "      <td>3.910000</td>\n",
       "      <td>100</td>\n",
       "      <td>92.280000</td>\n",
       "      <td>3.567149</td>\n",
       "      <td>female</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  avg_rating  avg_rating_customer  quantity_customer  \\\n",
       "0       1    2.550000             2.550000                 20   \n",
       "1       2    3.486842             3.486842                 76   \n",
       "2       3    3.568627             3.568627                 51   \n",
       "3       4    4.348039             4.348039                204   \n",
       "4       5    3.910000             3.910000                100   \n",
       "\n",
       "   seen_popularity  seen_rating  gender  age  \n",
       "0        45.550000     3.562529  female   39  \n",
       "1       106.578947     3.536646    male   35  \n",
       "2       116.843137     3.716531    male   36  \n",
       "3        72.480392     3.610074  female   24  \n",
       "4        92.280000     3.567149  female   32  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting user data\n",
    "users = pd.read_csv('customer.csv')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting item table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>avg_rating_item</th>\n",
       "      <th>quantity_item</th>\n",
       "      <th>likability</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.277838</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>Animation</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1029</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>42</td>\n",
       "      <td>0.156436</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1061</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>33</td>\n",
       "      <td>0.082347</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>1995</td>\n",
       "      <td>Romance</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1129</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.160192</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1172</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>46</td>\n",
       "      <td>0.646038</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  avg_rating_item  quantity_item  likability  \\\n",
       "0       31         3.178571             42   -0.277838   \n",
       "1     1029         3.702381             42    0.156436   \n",
       "2     1061         3.545455             33    0.082347   \n",
       "3     1129         3.312500             48   -0.160192   \n",
       "4     1172         4.260870             46    0.646038   \n",
       "\n",
       "                         title  year      genre  cost  \n",
       "0                    Toy Story  1995  Animation   149  \n",
       "1                      Jumanji  1995  Adventure   583  \n",
       "2             Grumpier Old Men  1995    Romance   450  \n",
       "3            Waiting to Exhale  1995     Comedy   839  \n",
       "4  Father of the Bride Part II  1995     Comedy   846  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting item data\n",
    "items = pd.read_csv('items.csv',encoding='latin1')\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Partitioning items into Head and Tail\n",
    "Break the total items I into head and tail part. We have selected breakdown point $\\alpha$ = 10 where $\\alpha$ means items rating frequency > $\\alpha$. In this case, the $\\alpha$ is 10. The item quantity \n",
    "- '>30' are labeled as 'head' and \n",
    "- <=30 are labeled as 'tail' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark movie popular or tail\n",
    "def cal_item_cat(row,break_point):\n",
    "    popularity = int(row['quantity_item'])\n",
    "    if(popularity > break_point):\n",
    "        return 'head'\n",
    "    return 'tail'\n",
    "\n",
    "def assign_item_cat(df,break_point):\n",
    "    df['item_cat'] = df.apply(lambda row: cal_item_cat(row,break_point), axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tail    8245\n",
       "head     821\n",
       "Name: item_cat, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tail breakdown is at 30\n",
    "items = assign_item_cat(items,30)\n",
    "items['item_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cluster tail items\n",
    "\n",
    "After identifying head and tail item, tail items T are clusterd using Expectation Maximization (EM) Clustering. EM is more likely to K-mean cluster model, but the advantage of EM is that it maximize the likelihood of data distribution in cluster by estimating the means and standard deviations(SD) for each cluster. Clustering featurs considered are\n",
    "- I_aver_rating \n",
    "- I_popularity\n",
    "- I_likablility \n",
    "from the derived variable list.\n",
    "\n",
    "Algorithm maps each tail T items into the 3-dimensional Euclidian space formed from above three variables and performs the clustering on that. \n",
    "\n",
    "Currently we have considered 10 number of clusters to be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting deta with 3 drived variables\n",
    "train_df = items[items['item_cat'] == 'tail'][['movieId','avg_rating_item','quantity_item','likability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=10)\n",
    "x = gmm.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'cluster': gmm.predict(train_df)}).cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing clusters\n",
    "items.loc[items.item_cat == 'tail','cluster'] = gmm.predict(items[items.item_cat == 'tail']\n",
    "                                                             [['movieId','avg_rating_item',\n",
    "                                                               'quantity_item','likability']]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigining head items as 11th cluster\n",
    "items.loc[items.item_cat == 'head','cluster'] = 10\n",
    "items['cluster'] = items['cluster'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>avg_rating_item</th>\n",
       "      <th>quantity_item</th>\n",
       "      <th>likability</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>cost</th>\n",
       "      <th>item_cat</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>42</td>\n",
       "      <td>-0.277838</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>Animation</td>\n",
       "      <td>149</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1029</td>\n",
       "      <td>3.702381</td>\n",
       "      <td>42</td>\n",
       "      <td>0.156436</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>583</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1061</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>33</td>\n",
       "      <td>0.082347</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>1995</td>\n",
       "      <td>Romance</td>\n",
       "      <td>450</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1129</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.160192</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>839</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1172</td>\n",
       "      <td>4.260870</td>\n",
       "      <td>46</td>\n",
       "      <td>0.646038</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>846</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1263</td>\n",
       "      <td>3.864583</td>\n",
       "      <td>48</td>\n",
       "      <td>0.316899</td>\n",
       "      <td>Heat</td>\n",
       "      <td>1995</td>\n",
       "      <td>Action</td>\n",
       "      <td>533</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1287</td>\n",
       "      <td>3.891304</td>\n",
       "      <td>46</td>\n",
       "      <td>0.391538</td>\n",
       "      <td>Sabrina</td>\n",
       "      <td>1995</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>419</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1293</td>\n",
       "      <td>3.978261</td>\n",
       "      <td>46</td>\n",
       "      <td>0.450087</td>\n",
       "      <td>Tom and Huck</td>\n",
       "      <td>1995</td>\n",
       "      <td>Action</td>\n",
       "      <td>1694</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1339</td>\n",
       "      <td>3.298077</td>\n",
       "      <td>52</td>\n",
       "      <td>-0.163060</td>\n",
       "      <td>Sudden Death</td>\n",
       "      <td>1995</td>\n",
       "      <td>Action</td>\n",
       "      <td>508</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1343</td>\n",
       "      <td>3.743590</td>\n",
       "      <td>39</td>\n",
       "      <td>0.207635</td>\n",
       "      <td>GoldenEye</td>\n",
       "      <td>1995</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>1142</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  avg_rating_item  quantity_item  likability  \\\n",
       "0       31         3.178571             42   -0.277838   \n",
       "1     1029         3.702381             42    0.156436   \n",
       "2     1061         3.545455             33    0.082347   \n",
       "3     1129         3.312500             48   -0.160192   \n",
       "4     1172         4.260870             46    0.646038   \n",
       "5     1263         3.864583             48    0.316899   \n",
       "6     1287         3.891304             46    0.391538   \n",
       "7     1293         3.978261             46    0.450087   \n",
       "8     1339         3.298077             52   -0.163060   \n",
       "9     1343         3.743590             39    0.207635   \n",
       "\n",
       "                         title  year      genre  cost item_cat  cluster  \n",
       "0                    Toy Story  1995  Animation   149     head       10  \n",
       "1                      Jumanji  1995  Adventure   583     head       10  \n",
       "2             Grumpier Old Men  1995    Romance   450     head       10  \n",
       "3            Waiting to Exhale  1995     Comedy   839     head       10  \n",
       "4  Father of the Bride Part II  1995     Comedy   846     head       10  \n",
       "5                         Heat  1995     Action   533     head       10  \n",
       "6                      Sabrina  1995     Comedy   419     head       10  \n",
       "7                 Tom and Huck  1995     Action  1694     head       10  \n",
       "8                 Sudden Death  1995     Action   508     head       10  \n",
       "9                    GoldenEye  1995  Adventure  1142     head       10  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data_final.csv',encoding='latin1')\n",
    "data = pd.merge(df,items[['movieId','cluster','item_cat']], on='movieId',how='inner')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Data Mining model for each cluster\n",
    "\n",
    "Next we use Random Forest Regression(RF) to predict new or unkown ratings. For each cluster this data mining models are trained and store to predict ratings of cluster items. \n",
    "\n",
    "We have performed grid serch to opt best parameter for the RF with 5 fold cross validation. And after that best performing parameters are chosen to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['userId','age','movieId','cost','avg_rating_item','quantity_item','likability',\n",
    "       'avg_rating_customer','quantity_customer','seen_popularity','seen_rating']\n",
    "\n",
    "def train_models(df):\n",
    "    model = []\n",
    "    for i in range(0,11):\n",
    "        features_df = df[df.cluster == i][col_list]\n",
    "        target_df = df[df.cluster == i]['rating']\n",
    "        features_df = pd.get_dummies(features_df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_df,target_df,\n",
    "                                                               test_size=0.3,shuffle=True)\n",
    "        tuned_parameters ={'max_features': ['sqrt','log2','auto'],\n",
    "                              'max_depth': [30,25,20,10,5],}\n",
    "        t1 = t.time()\n",
    "        gridsearch_rf = GridSearchCV(RandomForestRegressor(),tuned_parameters,cv =5, \n",
    "                                         scoring = 'neg_mean_squared_error')\n",
    "        gridsearch_rf.fit(X_train,y_train)\n",
    "        \n",
    "        print(\"##############################################################################\")\n",
    "        print(\"Gridsearch is completed for RF_\",i)\n",
    "        print(\"Time taken:\",round((t.time() - t1)/60,3),\"m\")\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(\"Best parameter choosen: {}\".format(gridsearch_rf.best_params_))\n",
    "        print(\"Mean Sequard Error: {}\".format(gridsearch_rf.best_score_))\n",
    "        print(\"Fitting on the entire training dataset using the best parameter found....\")\n",
    "        model.append(gridsearch_rf.best_estimator_)\n",
    "        model[i].fit(X_train,y_train)\n",
    "        print(\"Fitting completed.\")\n",
    "        print(model[i])\n",
    "        print(\"##############################################################################\")\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 0\n",
      "Time taken: 0.981 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 10, 'max_features': 'log2'}\n",
      "Mean Sequard Error: -0.691241438176717\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
      "           max_features='log2', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 1\n",
      "Time taken: 0.075 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'auto'}\n",
      "Mean Sequard Error: -0.6427524668110953\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 2\n",
      "Time taken: 0.069 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'auto'}\n",
      "Mean Sequard Error: -0.5930800658931165\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 3\n",
      "Time taken: 0.036 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'sqrt'}\n",
      "Mean Sequard Error: -0.7114973311188088\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 4\n",
      "Time taken: 0.069 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'auto'}\n",
      "Mean Sequard Error: -0.4984832063141001\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 5\n",
      "Time taken: 0.058 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'auto'}\n",
      "Mean Sequard Error: -0.49714577406920324\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 6\n",
      "Time taken: 0.093 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'log2'}\n",
      "Mean Sequard Error: -0.6502628055172927\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='log2', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 7\n",
      "Time taken: 0.046 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'sqrt'}\n",
      "Mean Sequard Error: -0.3839437288307689\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 8\n",
      "Time taken: 0.067 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 30, 'max_features': 'auto'}\n",
      "Mean Sequard Error: 0.0\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n",
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 9\n",
      "Time taken: 0.042 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 5, 'max_features': 'sqrt'}\n",
      "Mean Sequard Error: -0.5692671247197828\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################\n",
      "Gridsearch is completed for RF_ 10\n",
      "Time taken: 2.245 m\n",
      "---------------------------------------------------------------\n",
      "Best parameter choosen: {'max_depth': 10, 'max_features': 'sqrt'}\n",
      "Mean Sequard Error: -0.7113278601785751\n",
      "Fitting on the entire training dataset using the best parameter found....\n",
      "Fitting completed.\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
      "           max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "##############################################################################\n"
     ]
    }
   ],
   "source": [
    "models = train_models(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. COLLABORATIVE FILTERING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply collaborative filtering. In that first we will find 10 most similar user and items they have rated. After that we will drop common items rated by user for whome top N list you want and his other similar users. We will predict ratings for remaning items and retrive top N list from that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation\n",
    "\n",
    "Pearson Correlation Score:\n",
    "\n",
    "It is more sophisticated method to similarity between user intrest then simple cosine similarity. The correlation coefficient is a measure of how well two sets of data fit on a straight line. The formula for this is more complicated that the Euclidean distance score, but it tends to give better results in situations where the data isn’t well normalized like our present data set.\n",
    "\n",
    "$$ S_x = \\dfrac{\\sum x^2 - (\\sum x)^2}{n}$$<br>\n",
    "$$ S_y = \\dfrac{\\sum y^2 - (\\sum y)^2}{n}$$<br>\n",
    "$$ S_{xy} = \\dfrac{\\sum xy - (\\sum x)(\\sum x)}{n}$$<br>\n",
    "$$score = \\dfrac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}$$\n",
    "\n",
    "Implementation for the Pearson correlation score first finds the items rated by both users. It then calculates the sums and the sum of the squares of the ratings for the both users and calculates the sum of the products of their ratings. Finally, it uses these results to calculate the Pearson correlation coefficient.Unlike the distance metric, this formula is not intuitive, but it does tell you how much the variables change together divided by the product of how much they alter individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(user1,row,df):\n",
    "    # Getting user\n",
    "    user2 = int(row['userId'])\n",
    "    \n",
    "    # To get both rated items\n",
    "    u1_r = df[df['userId'] == user1][['movieId','rating']]\n",
    "    u2_r = df[df['userId'] == user2][['movieId','rating']]\n",
    "    both_rated = u1_r.merge(u2_r,on='movieId',how='inner')\n",
    "    \n",
    "    number_of_ratings = len(both_rated)\n",
    "    # Checking for number of ratings in common\n",
    "    if number_of_ratings == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Add up all the preferences of each user\n",
    "    user1_preferences_sum = both_rated['rating_x'].sum()\n",
    "    user2_preferences_sum = both_rated['rating_y'].sum()\n",
    "    \n",
    "    # Sum up the squares of preferences of each user\n",
    "    user1_square_preferences_sum = (both_rated['rating_x'] ** 2).sum()\n",
    "    user2_square_preferences_sum = (both_rated['rating_y'] ** 2).sum()\n",
    "    \n",
    "    # Sum up the product value of both preferences for each item\n",
    "    product_sum_of_both_users = (both_rated['rating_x'] * both_rated['rating_y']).sum()\n",
    "    \n",
    "    # Calculate the pearson score\n",
    "    numerator_value = product_sum_of_both_users - (user1_preferences_sum*user2_preferences_sum/number_of_ratings)\n",
    "    denominator_value = math.sqrt((user1_square_preferences_sum - pow(user1_preferences_sum,2)/number_of_ratings)\n",
    "                                 * (user2_square_preferences_sum -pow(user2_preferences_sum,2)/number_of_ratings))\n",
    "    \n",
    "    if denominator_value == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        r = numerator_value/denominator_value\n",
    "        return r\n",
    "\n",
    "def most_similar_users(user_df,rating_df,person,number_of_users):\n",
    "    \n",
    "    # returns the number_of_users (similar persons) for a given specific person.\n",
    "    correlation_data = user_df.copy()\n",
    "    correlation_data = correlation_data[correlation_data.userId != person]\n",
    "    correlation_data['relation_score'] = correlation_data.apply(lambda row: pearson_correlation(person,row,rating_df),\n",
    "                                                                axis = 1)\n",
    "    #print(data.head())\n",
    "    \n",
    "    # Sort the similar persons so that highest scores person will appear at the first\n",
    "    correlation_data = correlation_data.sort_values(by='relation_score', ascending=False)\n",
    "    return correlation_data[0:number_of_users].userId  #[['userId','relation_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_similar_users(users,data,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(row):\n",
    "    i = int(row['cluster'])\n",
    "    #print(row[col_list].values.tolist())\n",
    "    return models[i].predict([row[col_list].values.tolist()])[0]\n",
    "    \n",
    "    \n",
    "def get_ratings(df):\n",
    "    df['rating'] = df.apply(lambda row: predict_rating(row),axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_recommendation(person):\n",
    "    \n",
    "    # Getting top 10 similar user \n",
    "    sim_users = most_similar_users(users,data,person,10)\n",
    "    # Getting items that are rated by similar users \n",
    "    item_list = data.loc[data['userId'].isin(sim_users)]\n",
    "    \n",
    "    # Selecting only items that user haven't seen yet from item_list\n",
    "    user_items = data[data.userId == person].movieId.tolist()\n",
    "    recom_item_list = item_list.loc[~(item_list['movieId'].isin(user_items))].movieId.unique().tolist() \n",
    "    recom_item_data = items.loc[items['movieId'].isin(recom_item_list)] \n",
    "    \n",
    "    # Predicting ratings for items\n",
    "    recomm_data = recom_item_data.assign(key=1).merge(users[users['userId'] == person].assign(key=1)).drop('key', 1)\n",
    "    recomm_data = get_ratings(recomm_data)\n",
    "    \n",
    "    # Sort data by ratings \n",
    "    recomm_data = recomm_data.sort_values(by='rating', ascending=False)\n",
    "    return recomm_data#recomm_data[0:number_of_recommendation][['movieId','cost','item_cat','cluster','genre']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Recommendation for user 2\")\n",
    "#rec = user_recommendation(2).reset_index(drop=True)\n",
    "#rec.head(10)\n",
    "#len(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CEAM FILTERING\n",
    "\n",
    "1. We will be creating a database with average and median prices of items within different categories. In order to have a comparison of particular user’s spending with average or median of different categories, we will need data of previous purchases by a customer.\n",
    "2. Now in order to compare it with previous purchases of a customer, we need to make calculations of Average Ratio. Average Ratio is the proportion of price of the items bought by users and the average price of the categories of items that the customer has bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "items['genre'] = items['genre'].fillna('unknown')\n",
    "data['genre'] = data['genre'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = pd.DataFrame({'genre' : items.genre.unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_avg_price(row,cost_data):\n",
    "    cat = row['genre']\n",
    "    #print(cat)\n",
    "    cost_data = cost_data[cost_data.genre == cat]\n",
    "    return cost_data['cost'].sum()/len(cost_data)\n",
    "\n",
    "def avg_cat_price(cat_data,cost_data):\n",
    "    cat_data['avg_cost'] = cat_data.apply(lambda row: cal_avg_price(row,cost_data), axis=1)\n",
    "    return cat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df = avg_cat_price(genre_df,items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_purchase_score(row):\n",
    "    category = row['genre']\n",
    "    price = int(row['cost'])\n",
    "    #print(genre_df[genre_df.genre == category]['avg_cost'])\n",
    "    avg_cost = int(genre_df[genre_df.genre == category]['avg_cost'])\n",
    "    return price/avg_cost\n",
    "\n",
    "def purchase_score(row):\n",
    "    temp = data[data.userId == int(row['userId'])]\n",
    "    temp['purchase_score'] = temp.apply(lambda row: cal_purchase_score(row), axis=1)\n",
    "    return temp.purchase_score.sum()/len(temp)\n",
    "\n",
    "def avg_purchase(df):\n",
    "    df['avg_purchase_score'] = df.apply(lambda row: purchase_score(row), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average spending ration for each user\n",
    "users = avg_purchase(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_score(df):\n",
    "    df['score'] = df.apply(lambda row: cal_purchase_score(row), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N(user,N):\n",
    "    # Getting collobrativ filtered data\n",
    "    rec = user_recommendation(user).reset_index(drop=True)\n",
    "    \n",
    "    # Getting cost ratio of items to be reommended \n",
    "    rec = item_score(rec)\n",
    "    num = int(users[users.userId == 2].avg_purchase_score)\n",
    "    rec = rec[rec['score'].between(num-1,num+1,inclusive=True)]\n",
    "    \n",
    "    # Getting top-N items\n",
    "    top_N = (rec.sort_values(by='rating', ascending=False)).reset_index(drop=True).head(N)\n",
    "    print(top_N['title'])\n",
    "    print(\"------------------------------\")\n",
    "    print(top_N.item_cat.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Batman: Mask of the Phantasm\n",
      "1              Send Me No Flowers\n",
      "2                    Doctor Sleep\n",
      "3              The Usual Suspects\n",
      "4               Leaving Las Vegas\n",
      "5                      Diabolique\n",
      "6                         Khomreh\n",
      "7                    Urban Legend\n",
      "8           Come See the Paradise\n",
      "9                   Anna Karenina\n",
      "Name: title, dtype: object\n",
      "------------------------------\n",
      "tail    6\n",
      "head    4\n",
      "Name: item_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# making top 10 recommendation for userid 661\n",
    "top_N(661,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
